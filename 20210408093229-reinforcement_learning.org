#+title: Reinforcement learning
#+PROPERTY: header-args :tangle rl/agentClass.py

#+call: init()

#+RESULTS:

* Q-learning
Q-learning is an algorithm used for reinforcement learning where the main goal
is to find the best action given the state the agent is in. Each state and
action maps to a /Q-value/, the action with the highest value represents the
best action the agent can take for the current state.

** Q-learning with state-action table
The simplest implementation of Q-learning is using a state-action table or a
/Q-table/, where each cell contains a Q-value. The rows represents states and
each column represents actions.

# Plotting imports
#+begin_src jupyter-python :tangle no
import numpy as np
from scipy.ndimage.filters import uniform_filter1d
import matplotlib.pyplot as plt
from importlib import reload
from encyclopedia.latex import *
from sympy import *
#+end_src

#+RESULTS:

# Agent imports
#+begin_src jupyter-python
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import pytorch_lightning as pl
from sys import exit
import random
import copy
import math
import h5py
#+end_src

#+RESULTS:

The first thing we need to do is calculate the number of possible states and
actions to be able to create the Q-table. We know that each tile of the board
can only have two possible states and we have four tile types, which means that
we can calculate the number of states by $2^{\text{cols}\cdot\text{rows}}\cdot
\text{tiles}=2^{16}\cdot 4=2^{18}=262144$ states.

#+begin_src jupyter-python :exports code
def calculate_possibilities(gameboard):
    board_bits = gameboard.N_row*gameboard.N_col
    board_states = 2**board_bits
    tile_states = len(gameboard.tiles)
    N_states = board_states * tile_states

    orientation_actions = 4
    x_pos_actions = gameboard.N_col
    N_actions = x_pos_actions*orientation_actions

    return N_states, N_actions
#+end_src

#+RESULTS:

#+begin_src jupyter-python :exports code
def encode_state(board, cur_tile_type):
    # Convert board to binary
    b = ((board + 1) / 2).flatten().astype(int)
    # Convert binary board array to number
    board_state = b.dot(2**np.arange(b.size))
    # Shift board state two bits and place current tile state there
    state = (board_state << 2) | cur_tile_type

    return state
#+end_src

#+RESULTS:

#+begin_src jupyter-python :exports code
def decode_action(action):
    # Take the first two bits for tile_x
    tile_x = action & 0b11
    # then the second two bits for orientation
    tile_orientation = (action >> 2) & 0b11

    return tile_x, tile_orientation
#+end_src

#+begin_src jupyter-python :exports code
def get_valid_actions(gameboard, N_actions):
    valid_actions = np.zeros(N_actions, dtype=bool)
    for action in range(N_actions):
        tile_x, tile_orientation = decode_action(action)
        invalid = gameboard.fn_move(tile_x, tile_orientation)
        valid_actions[action] = invalid == 0
    return valid_actions
#+end_src

#+begin_src jupyter-python :exports code :results silent
def get_new_action(q_values, valid_actions, epsilon):
    if np.random.random() > epsilon:
        q_values[~valid_actions] = -np.inf
        # Find all maximum Q-values (in case some are equal)
        q_valid_max_i = np.where(q_values == q_values.max())[0]
        # Return the action with max-Q (if multiple equal return random one)
        return np.random.choice(q_valid_max_i)
    else:
        # Return a random valid action
        return np.random.choice(np.where(valid_actions == True)[0], replace=False)
#+end_src

#+begin_src jupyter-python :tangle no
get_new_action(np.array([3.0, 1.0, 2.0, 4.0, 6.0]), np.array([0, 0, 1, 0, 0]).astype(bool), 0.0)
#+end_src

#+RESULTS:
: 2

We will now define a class for the agent
#+begin_src jupyter-python :exports code
class TQAgent:
    # Agent for learning to play tetris using Q-learning
    def __init__(self,alpha,epsilon,episode_count):
        self.alpha = alpha
        self.epsilon = epsilon
        self.episode = 0
        self.episode_count = episode_count
#+end_src

we can now define a function to initialize and reset the state for the agent
#+begin_src jupyter-python :exports code
# >>> class TQAgent:
    def fn_init(self, gameboard):
        self.gameboard = gameboard
        self.N_states, self.N_actions = calculate_possibilities(gameboard)
        self.q_table = np.zeros([self.N_states, self.N_actions])
        self.reward_tots = np.zeros(self.episode_count)
        self.current_state = 0
        self.current_action = 0
#+end_src

we also define a function for loading a Q-table
#+begin_src jupyter-python :exports code
# >>> class TQAgent:
    def fn_load_strategy(self, strategy_file):
        self.q_table = np.load(strategy_file)
#+end_src

#+begin_src jupyter-python :exports code
# >>> class TQAgent:
    def fn_read_state(self):
        self.current_state = encode_state(self.gameboard.board,
                                          self.gameboard.cur_tile_type)
#+end_src

Selecting an action has three steps, find valid actions, get a new action using
the Q-values and finally decoding the action to move the tile.
#+begin_src jupyter-python :exports code
# >>> class TQAgent:
    def fn_select_action(self):
        valid_actions = get_valid_actions(self.gameboard, self.N_actions)
        # Find valid action q-values for current state
        q_values = self.q_table[self.current_state]
        self.current_action = get_new_action(q_values,
                                            valid_actions,
                                            self.epsilon)
        tile_x, tile_orientation = decode_action(self.current_action)
        self.gameboard.fn_move(tile_x, tile_orientation)
#+end_src

#+begin_src jupyter-python :exports code
# >>> class TQAgent:
    def fn_reinforce(self,old_state,reward):
        Q = self.q_table[old_state, self.current_action]
        Q_max = self.q_table[self.current_state].max()
        Q_new = Q + self.alpha * (reward + Q_max-Q)
        self.q_table[old_state, self.current_action] = Q_new
#+end_src

Lastly we combine it all into a step function that selects an action, executes
that action and collects the reward from the environment, then reading the
resulting state and finally updating the Q-table.
#+begin_src jupyter-python :exports code
# >>> class TQAgent:
    def fn_step(self):
        # Select and execute action (move the tile to the desired column and orientation)
        self.fn_select_action()
        old_state = self.current_state

        # Drop the tile on the game board
        reward = self.gameboard.fn_drop()
        self.reward_tots[self.episode] += reward

        # Read the new state
        self.fn_read_state()
        # Update the Q-table using the old state and the reward
        self.fn_reinforce(old_state,reward)
#+end_src

#+begin_src jupyter-python
# >>> class TQAgent:
    def fn_turn(self):
        if self.gameboard.gameover:
            self.episode += 1
            if self.episode % 100 == 0:
                mean_reward = np.mean(self.reward_tots[self.episode-100:self.episode])
                print(f"episode {self.episode}/{self.episode_count} (reward: {mean_reward})")
            if self.episode % 1000 == 0:
                np.save(f'tqagent_table.npy', self.q_table)
                np.save(f'tqagent_rewards.npy', self.reward_tots)
            if self.episode >= self.episode_count:
                exit(0)
            else:
                self.gameboard.fn_restart()
        else:
            # Select and execute action (move the tile to the desired column and orientation)
            self.fn_step()
#+end_src

*** Greedy action selection
The first experiment will use greedy action selection, by finding the action
with the highest Q-value. To do this, you simply go to the row that represent
the current state and find the action with the highest value.

#+begin_src jupyter-python :tangle no
a = IndexedBase('a')
s = IndexedBase('s')
eps = IndexedBase('varepsilon')
Qt = Function('Q_t')
t, E = symbols('t E')
LEq(a[t], LArgmax(a, Qt(s[t], a)))
#+end_src

#+RESULTS:
:RESULTS:
\begin{equation}{a_{t}}=\operatorname{argmax}_{a}{\operatorname{Q_{t}}{\left({s_{t}},a \right)}}\end{equation}
:END:

#+begin_src jupyter-python :results output :tangle no
plt.figure(figsize=(4, 3))
rewards = np.load('tqagent_rewards_1a.npy')
plt.plot(rewards, lw=0.8, label="reward")
plt.plot(uniform_filter1d(rewards, size=100), lw=0.8, label="moving average")
plt.xlabel("E")
plt.ylabel("R")
plt.legend()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/669a0e100ac3717bc68b89f6877c6148da4e8a98.png]]

*** Epsilon greedy action selection
When using the greedy action selection we can easily get stuck in a local
minima. To fix this we can introduce some randomness to action selection, we
call this the /epsilon greedy/ action selection. We introduce a parameter
$\varepsilon$ that represents the probability of taking a random action.
#+begin_src jupyter-python :tangle no
LEq(a[t], LPiecewise(
    'random valid action', lamp, 'with probability ', eps[E], lbreak,
    LArgmax(a, Qt(s[t], a)), lamp, 'otherwise'
))
#+end_src

#+RESULTS:
:RESULTS:
\begin{equation}{a_{t}}=\begin{cases}
\mathtt{\text{random valid action}}&\mathtt{\text{with probability }}{\varepsilon_{E}}\\
\operatorname{argmax}_{a}{\operatorname{Q_{t}}{\left({s_{t}},a \right)}}&\mathtt{\text{otherwise}}
\end{cases}\end{equation}
:END:

#+begin_src jupyter-python :results output :tangle no
plt.figure(figsize=(4, 3))
rewards = np.load('tqagent_rewards_1b.npy')
plt.plot(rewards, lw=0.4, label="reward")
plt.plot(uniform_filter1d(rewards, size=100), lw=0.8, label="moving average")
plt.xlabel("E")
plt.ylabel("R")
plt.legend()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/b320b9f4195b134d2b0f991744f5abd7fdcf173a.png]]

*** Epsilon greedy with random tile sequence
#+begin_src jupyter-python :results output :tangle no
plt.figure(figsize=(4, 3))
rewards = np.load('tqagent_rewards_1c.npy')
plt.plot(rewards, lw=0.4, label="reward")
plt.plot(uniform_filter1d(rewards, size=100), lw=0.4, label="moving average")
plt.xlabel("E")
plt.ylabel("R")
plt.legend()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/d82124ffdf295e7eb63df26e4d8ba74bce7c6aee.png]]

*** Larger game board
If we were to use a larger board like 8x8, the number of states would be much
larger. If each square is either occupied by a tile or not occupied we have
$2^{8\cdot 8}=2^{64}\approx 1.84\cdot 10^{19}$ states only for the board. This
size of table is not feasible since if we use a 32-bit float for each Q-value
the table would be taking up exabytes of memory.

** Deep Q-learning using a neural network
To overcome the explosion of states we can instead approximate some of the
states. One way to do this is to replace the Q-table with a deep neural network.
Here the hope is that the network will be able to generalize over similar or
equivalent states, effectively reducing the number of states that the agent have
to search.

#+begin_src jupyter-python :exports code
class QModel(pl.LightningModule):
    def __init__(self, batch_size, board_shape, tile_id_size, hidden_channels, num_actions):
        super(QModel, self).__init__()
        # torch.manual_seed(12348)
        self.lin1 = nn.Linear(tile_id_size + board_shape[0]*board_shape[1], hidden_channels)
        self.lin2 = nn.Linear(hidden_channels, num_actions)

    def forward(self, x):
        x = F.relu(self.lin1(x))
        x = self.lin2(x)
        return x
#+end_src

#+begin_src jupyter-python :exports code
class TDQNAgent:
    # Agent for learning to play tetris using Q-learning
    def __init__(self,
                 alpha,
                 epsilon,
                 epsilon_scale,
                 replay_buffer_size,
                 batch_size,
                 sync_target_episode_count,
                 episode_count):
        # Initialize training parameters
        self.alpha = alpha
        self.epsilon = epsilon
        self.epsilon_scale = epsilon_scale
        self.replay_buffer_size = replay_buffer_size
        self.batch_size = batch_size
        self.sync_target_episode_count = sync_target_episode_count
        self.episode = 0
        self.episode_count = episode_count
        self.hidden_channels = 64
#+end_src

#+begin_src jupyter-python :exports code
# >>> class TDQNAgent:
    def fn_init(self, gameboard):
        self.gameboard = gameboard
        self.N_actions = self.gameboard.N_col*4
        self.board_shape = (gameboard.N_row, gameboard.N_col)
        self.tile_id_size = len(gameboard.tiles)
        self.reward_tots = np.zeros(self.episode_count)
        self.state_size = self.board_shape[0]*self.board_shape[1]+self.tile_id_size

        self.q_model = QModel(self.batch_size,
                              self.board_shape,
                              self.tile_id_size,
                              self.hidden_channels,
                              self.N_actions)
        self.q_model_target = QModel(self.batch_size,
                                     self.board_shape,
                                     self.tile_id_size,
                                     self.hidden_channels,
                                     self.N_actions)

        self.q_model_target.load_state_dict(self.q_model.state_dict())
        self.q_model_target.eval()
        self.optimizer = torch.optim.Adam(self.q_model.parameters(), lr=self.alpha)

        self.current_state = torch.zeros(self.state_size).view(1, -1)-1
        self.exp_buffer = []
#+end_src

#+begin_src jupyter-python :exports code
# >>> class TDQNAgent:
    def fn_load_strategy(self, strategy_file):
        pass
#+end_src

#+begin_src jupyter-python :exports code
# >>> class TDQNAgent:
    def fn_read_state(self):
        board = torch.tensor(self.gameboard.board).view(-1)
        tile = (F.one_hot(torch.tensor(self.gameboard.cur_tile_type), self.tile_id_size)*2-1)
        self.current_state = torch.cat([board, tile]).view(1, -1)
#+end_src

First, we will have to change the action selection since we aren't using a table
anymore

#+begin_src jupyter-python :exports code
# >>> class TDQNAgent:
    def fn_select_action(self):
        valid_actions = get_valid_actions(self.gameboard, self.N_actions)
        epsilon_E = max(self.epsilon, 1-self.episode/self.epsilon_scale)
        # Find valid action q-values for current state
        with torch.no_grad():
            q_values = self.q_model(self.current_state).view(-1).detach().numpy()
        self.current_action = get_new_action(q_values,
                                                valid_actions,
                                                epsilon_E)
        assert(self.current_action in np.where(valid_actions)[0])
        tile_x, tile_orientation = decode_action(self.current_action)
        self.gameboard.fn_move(tile_x, tile_orientation)
#+end_src

#+begin_src jupyter-python :exports code
# >>> class TDQNAgent:
    def fn_reinforce(self, batch):
        old_state, action, reward, cur_state, final = batch

        q_value = self.q_model(old_state).gather(1, action.view(-1, 1)).squeeze()
        target_q_values = self.q_model_target(cur_state).max(dim=1)[0].detach()

        y = reward + target_q_values*(1-final)
        loss = torch.sum((q_value-y)**2)

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
#+end_src

#+begin_src jupyter-python :exports code
# >>> class TDQNAgent:
    def fn_step(self):
        # Select and execute action (move the tile to the desired column and orientation)
        self.fn_select_action()
        old_state = self.current_state.clone()

        # Drop the tile on the game board
        reward = self.gameboard.fn_drop()
        self.reward_tots[self.episode] += reward

        # Read the new state
        self.fn_read_state()

        self.exp_buffer.append((old_state,
                                self.current_action,
                                reward,
                                self.current_state,
                                self.gameboard.gameover))

        if len(self.exp_buffer) >= self.replay_buffer_size:
            batch = random.sample(self.exp_buffer, self.batch_size)
            old_state, action, reward, cur_state, final = tuple(zip(*batch))
            batch = (torch.cat(old_state),
                     torch.tensor(action),
                     torch.tensor(reward),
                     torch.cat(cur_state),
                     torch.tensor(final))
            self.fn_reinforce(batch)
            self.exp_buffer.pop(0)

            if self.episode % self.sync_target_episode_count == 0:
                self.q_model_target.load_state_dict(self.q_model.state_dict())
#+end_src

#+begin_src jupyter-python
    def fn_turn(self):
        if self.gameboard.gameover:
            self.episode += 1
            if self.episode % 100 == 0:
                mean_reward = np.mean(self.reward_tots[self.episode-100:self.episode])
                print(f"episode {self.episode}/{self.episode_count} (reward: {mean_reward})")
            if self.episode % 1000 == 0:
                # np.save(f'tqagent_table.npy', self.q_table)
                np.save(f'tdqnagent_rewards.npy', self.reward_tots)
            if self.episode >= self.episode_count:
                exit(0)
            else:
                self.gameboard.fn_restart()
        else:
            self.fn_step()
#+end_src

#+begin_src jupyter-python :results output :tangle no
plt.figure(figsize=(4, 3))
rewards = np.load('tdqnagent_rewards.npy')
plt.plot(rewards, lw=0.8, label="reward")
plt.plot(uniform_filter1d(rewards, size=100), lw=0.8, label="moving average")
plt.xlabel("E")
plt.ylabel("R")
plt.legend()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/01dc06ecd21056930acabd2d6a3057c2ffdd9e87.png]]

* Human agent :noexport:
#+begin_src jupyter-python
class THumanAgent:
    def fn_init(self,gameboard):
        self.episode=0
        self.reward_tots=[0]
        self.gameboard=gameboard

    def fn_read_state(self):
        pass

    def fn_turn(self,pygame):
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                pygame.quit()
                raise SystemExit(0)
            if event.type==pygame.KEYDOWN:
                if event.key == pygame.K_ESCAPE:
                    self.reward_tots=[0]
                    self.gameboard.fn_restart()
                if not self.gameboard.gameover:
                    if event.key == pygame.K_UP:
                        self.gameboard.fn_move(self.gameboard.tile_x,(self.gameboard.tile_orientation+1)%len(self.gameboard.tiles[self.gameboard.cur_tile_type]))
                    if event.key == pygame.K_LEFT:
                        self.gameboard.fn_move(self.gameboard.tile_x-1,self.gameboard.tile_orientation)
                    if event.key == pygame.K_RIGHT:
                        self.gameboard.fn_move(self.gameboard.tile_x+1,self.gameboard.tile_orientation)
                    if (event.key == pygame.K_DOWN) or (event.key == pygame.K_SPACE):
                        self.reward_tots[self.episode]+=self.gameboard.fn_drop()

#+end_src
