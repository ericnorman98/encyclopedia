#+title: Stochastic sis model
#+roam_tags: sis model stochastic population dynamic system

#+call: init()

#+begin_src jupyter-python
import matplotlib.pyplot as plt
import numpy as np
from sympy import *
from sympy.stats import *
from pyorg.latex import *
from encyclopedia.deterministic_sis_model import *
from scipy.optimize import curve_fit
from scipy.stats import gaussian_kde
from scipy.stats import norm
#+end_src

#+RESULTS:

* Stochastic sis model
The definition of the stochastic version of the sis model is a [[file:20210308084322-markov_chain.org][Markov chain]] and
can be described like this
#+begin_src jupyter-python
N, TI, TR, m = symbols('N T_I T_R m', integer=True)
alpha, beta, t = symbols('alpha beta t', real=True, positive=True)
n = Idx('n', m)
dt = Symbol('dt')
b, d, c, Pb, Pd = symbols('b d c P_b P_d', shape=(1,), cls=IndexedBase)
trns = lambda tr, op, rate: LBiOp(tr, op, rate, separator=':')
deterministic_model = DeterministicSISModel()

rates_lhs = Matrix([b[n], d[n]])
rates_rhs = Matrix([
    alpha*n*(1-n/N),
    beta*n,
])
nexts = Matrix([
    Lambda(n, n+1),
    Lambda(n, n-1),
])

LArray(rates_lhs, rates_rhs).transpose(LEq)
#+end_src

#+RESULTS:
:RESULTS:
\begin{equation}\begin{array}{l}
{b_{n}}=\alpha \left(1 - \frac{n}{N}\right) n\\
{d_{n}}=\beta n
\end{array}\end{equation}
:END:

where $n$ is $I_t$ for simplicity. The expression to the left of the colon
defines an event and the right side is the rate of the event occurring.

** Deriving deterministic model and master equation
The /master equation/ of the system looks like the following
#+begin_src jupyter-python
rho = symbols('rho', cls=Function)
Ep, Em, Epm = symbols('E^+ E^- E^{\\pm}')
master_dt = LEq(rho(n, t+dt), rho(n, t)+dt*((b[n-1]*rho(n-1,t)+d[n+1]*rho(n+1,t))-(b[n]*rho(n, t)+d[n]*rho(n, t))))
master_eq = LArray(
    lambda e: e/dt,
    LSimplifyStep(),
    LSubsStep(rho(n, t+dt)/dt, rho(n, t).diff(t)),
    LSubsStep(rho(n, t)/dt, 0)
).steps(master_dt)[-1]
master_eq.expand(rho(n,t))#.subs([(rho(n-1,t)*b[n-1],Em*rho(n,t)*b[n]), (rho(n+1,t)*d[n+1],Ep*rho(n,t)*d[n])]).factor(b[n],d[n])
#+end_src

#+RESULTS:
:RESULTS:
\begin{equation}\frac{d}{d t} \rho{\left(n,t \right)}=\rho{\left(n - 1,t \right)} {b_{n - 1}} + \rho{\left(n + 1,t \right)} {d_{n + 1}} - \rho{\left(n,t \right)} {b_{n}} - \rho{\left(n,t \right)} {d_{n}}\end{equation}
:END:

We can rewrite this equation using /step operators/ like below
#+begin_src jupyter-python
master_step = LEq(master_eq.lhs, (1-Ep)*rho(n,t)*b[n]+(1-Em)*rho(n,t)*d[n])
master_step
#+end_src

#+RESULTS:
:RESULTS:
\begin{equation}\frac{d}{d t} \rho{\left(n,t \right)}=\left(1 - E^{+}\right) \rho{\left(n,t \right)} {b_{n}} + \left(1 - E^{-}\right) \rho{\left(n,t \right)} {d_{n}}\end{equation}
:END:

To simplify the expansion of the equation, we introduce $\hat{I}=n/N$ and get $b(\hat{I})$
and $d(\hat{I})$, substituting this we get
#+begin_src jupyter-python
Ih = Symbol('\\hat{I}')
bf, df = symbols('b d', cls=Function)
master_subs = LComposeStep(
    # LSubsStep(list(zip(rates_lhs, rates_rhs))),
    # LSubsStep(list(zip(rates_lhs.subs(n, n-1), rates_rhs.subs(n, n-1)))),
    # LSubsStep(list(zip(rates_lhs.subs(n, n+1), rates_rhs.subs(n, n+1)))),
    LSubsStep([(b[n], bf(Ih)), (d[n], df(Ih))]),
    LSubsStep(n/N, Ih),
    # LSimplifyStep()
)(master_step)
master_subs
#+end_src

#+RESULTS:
:RESULTS:
\begin{equation}\frac{d}{d t} \rho{\left(n,t \right)}   =   \left(1 - E^{+}\right) b{\left(\hat{I} \right)} \rho{\left(n,t \right)} + \left(1 - E^{-}\right) d{\left(\hat{I} \right)} \rho{\left(n,t \right)}\end{equation}
:END:

We can represent the action of $E^+$ and $E^-$ on a smooth function $g(I)$ like
this
#+begin_src jupyter-python
g = Function('g')
k = Idx('k')
dIop = Symbol('\\frac{\\partial}{\\partial \\hat{I}}')
Epm_eq = lambda E, pm: LEq(E*g(Ih),
                           g(Ih+pm*1/N),
                           Sum(((pm/N)/factorial(k))*g(Ih).diff((Ih, k)), (k, 0, oo)),
                           exp(pm*(1/N)*dIop)*g(Ih))
LArray(Epm_eq(Ep, 1), Epm_eq(Em, -1))
#+end_src

#+RESULTS:
:RESULTS:
\begin{equation}\begin{array}{l}
E^{+} g{\left(\hat{I} \right)} = g{\left(\hat{I} + \frac{1}{N} \right)} = \sum_{k=0}^{\infty} \frac{\frac{d^{k}}{d \hat{I}^{k}} g{\left(\hat{I} \right)}}{N k!} = g{\left(\hat{I} \right)} e^{\frac{\frac{\partial}{\partial \hat{I}}}{N}}\\
E^{-} g{\left(\hat{I} \right)} = g{\left(\hat{I} - \frac{1}{N} \right)} = \sum_{k=0}^{\infty} - \frac{\frac{d^{k}}{d \hat{I}^{k}} g{\left(\hat{I} \right)}}{N k!} = g{\left(\hat{I} \right)} e^{- \frac{\frac{\partial}{\partial \hat{I}}}{N}}
\end{array}\end{equation}
:END:

Expanding using these definitions we get
#+begin_src jupyter-python
master_action = master_subs.subs([(Ep, Epm_eq(Ep, 1).rhs), (Em, Epm_eq(Em, -1).rhs)]).subs(g(Ih), 1)
master_action
#+end_src

#+RESULTS:
:RESULTS:
\begin{equation}\frac{d}{d t} \rho{\left(n,t \right)}     =     \left(1 - e^{- \frac{\frac{\partial}{\partial \hat{I}}}{N}}\right) d{\left(\hat{I} \right)} \rho{\left(n,t \right)} + \left(1 - e^{\frac{\frac{\partial}{\partial \hat{I}}}{N}}\right) b{\left(\hat{I} \right)} \rho{\left(n,t \right)}\end{equation}
:END:

This equation corresponds to a /transport equation/ an is approximately equal to
#+begin_src jupyter-python
master_trans = LApprox(master_action.lhs, Derivative(df(Ih)-bf(Ih), Ih)*rho(n,t))
master_trans
#+end_src

#+RESULTS:
:RESULTS:
\begin{equation}\frac{d}{d t} \rho{\left(n,t \right)} \approx \rho{\left(n,t \right)} \frac{d}{d \hat{I}} \left(- b{\left(\hat{I} \right)} + d{\left(\hat{I} \right)}\right)\end{equation}
:END:

The transport equation formulated in deterministic dynamics looks like this
#+begin_src jupyter-python
master_dynamic = LEq(Derivative(Ih, t), df(Ih)-bf(Ih), rates_rhs[0].subs(n/N, Ih)-rates_rhs[1].subs(n/N, Ih))
master_dynamic
#+end_src

#+RESULTS:
:RESULTS:
\begin{equation}\frac{d}{d t} \hat{I} = - b{\left(\hat{I} \right)} + d{\left(\hat{I} \right)} = \alpha \left(1 - \hat{I}\right) n - \beta n\end{equation}
:END:

Substituting $\hat{I}=n/N=I(t)/N=\frac{I(t)}{I(t)+S(t)}$ back into the equation we get
#+begin_src jupyter-python
LSteps(master_dynamic.lhs, col_join=LEq)(
    LSubsStep(master_dynamic.lhs, master_dynamic.rhs),
    LSubsStep(Ih, n/N),
    LSubsStep(n, I(t)),
    LSubsStep(N, I(t)+S(t)),
    LStep(lambda e: e.apart(alpha)),
    I(t).diff(t)
)
#+end_src

#+RESULTS:
:RESULTS:
\begin{equation}\begin{array}{l}
\frac{d}{d t} \hat{I} = \\
\quad = \alpha \left(1 - \hat{I}\right) n - \beta n  =  \\
\quad = \alpha \left(1 - \frac{n}{N}\right) n - \beta n   =   \\
\quad = \alpha \left(1 - \frac{I{\left(t \right)}}{N}\right) I{\left(t \right)} - \beta I{\left(t \right)}    =    \\
\quad = \alpha \left(1 - \frac{I{\left(t \right)}}{I{\left(t \right)} + S{\left(t \right)}}\right) I{\left(t \right)} - \beta I{\left(t \right)}     =     \\
\quad = \frac{\alpha I{\left(t \right)} S{\left(t \right)}}{I{\left(t \right)} + S{\left(t \right)}} - \beta I{\left(t \right)}      =      \\
\quad = \frac{d}{d t} I{\left(t \right)}
\end{array}\end{equation}
:END:
which is the same as the [[file:20210303161107-deterministic_sis_model.org][deterministic sis model]].

However, this does not mean that the stochastic model will behave exactly like
the deterministic model, the stochastic model actually has a /quasi-steady
states/. This means that some steady states seems to be stable for a while but
can suddenly diverge. We will investigate this in the next section when we
simulate the stochastic model.

** Simulation
To simulate the Markov chain, we need to know the probability going from one
state of $n$ to another. A simple method is to just multiply each rate by a
small time step like below

#+begin_src jupyter-python
nexts_full = nexts.row_insert(2, Matrix([Lambda(n, Add(n, 0, evaluate=False))]))
rates_rhs_full = rates_rhs.row_insert(2, Matrix([b[n]+d[n]]))
rates_lhs_full = rates_lhs.row_insert(2, Matrix([b[n]+d[n]]))
prob_lhs = nexts_full.applyfunc(Probability)
prob_rhs = rates_lhs_full*dt
prob_rhs[2] = 1-prob_rhs[2]
LMatEq(prob_lhs, prob_rhs)
#+end_src

#+RESULTS:
:RESULTS:
\begin{equation}\begin{array}{l}
P[\left( n \mapsto n + 1 \right)] = dt {b_{n}}\\
P[\left( n \mapsto n - 1 \right)] = dt {d_{n}}\\
P[\left( n \mapsto n + 0 \right)] = - dt \left({b_{n}} + {d_{n}}\right) + 1
\end{array}\end{equation}
:END:

Using the probabilities, we can simulate an event happening by comparing the
probability with a uniform random variable $r$ in the range $(0, 1)$. See Figure
[[fig:stoch_simulation]] for a simulation of the system compared to the
deterministic model. If we run the model for longer (see Figure
[[fig:stoch_quasi]]), we can see that it is only a quasi-steady state since the
infected population eventually dies out.

#+begin_src jupyter-python :exports none
prob_bd = lambdify((b[n], d[n], n, N, dt), prob_rhs)
prob_bd(0.1, 0.2, 2, 10, 0.1).shape
#+end_src

#+RESULTS:
| 3 | 1 |

#+begin_src jupyter-python :exports none
prob = lambdify((alpha, beta, n, N, dt), prob_rhs.subs(zip(rates_lhs, rates_rhs)))
prob(0.5, 0.1, 2, 10, 0.1)
#+end_src

#+RESULTS:
: array([[0.08],
:        [0.02],
:        [0.9 ]])

#+begin_src jupyter-python
def step(alpha, beta, n, N, dt, prob_n):
    should_transition = np.random.random(size=prob_n.shape) < prob_n
    for i, nn in zip(range(3), [1, -1, 0]):
        n += should_transition[i]*nn
    return n
#+end_src

#+RESULTS:

#+begin_src jupyter-python :results silent
def run_stoch_model(steps, V):
    V[n] = V[I0]
    V[S0] = V[N] - V[I0]
    n_n = np.array([V[n], V[n]])
    n_hist = [n_n.copy()]
    times = [0]
    t_n = 0
    for _ in range(steps):
        prob_n = np.squeeze(prob(V[alpha], V[beta], n_n, V[N], V[dt]))
        t_n += V[dt]
        step(V[alpha], V[beta], n_n, V[N], V[dt], prob_n)
        n_hist.append(n_n.copy())
        times.append(t_n)

    times = np.array(times)
    n_hist = np.array(n_hist)
    return times, n_hist
#+end_src

#+begin_src jupyter-python :results silent :noweb yes
def plot_stoch_model(times, n_hist, V):
    plt.plot(times, V[N]-n_hist[:, 0], color=<<color("green")>>, label="S stochastic", lw=0.5)
    plt.plot(times, n_hist[:, 0], color=<<color("red")>>, label="I stochastic", lw=0.5)
    deterministic_model.plot([0, len(times)*V[dt]], V, ls='--', alpha=0.8, lw=1.0)
#+end_src

#+name: src:fig:stoch_simulation
#+begin_src jupyter-python :noweb yes :results output :eval never-export
V = {
    N: 100,
    I0: 10,
    alpha: 0.5,
    beta: 0.1,
    dt: 0.05
}
steps = 1000
times, n_hist = run_stoch_model(steps, V)
plot_stoch_model(times, n_hist, V)
plt.xlabel("$t$")
plt.ylabel("population")
plt.legend()
#+end_src

#+caption: A simulation of the stochastic system compared to the deterministic one.
#+attr_latex: scale=0.75
#+label: fig:stoch_simulation
#+RESULTS: src:fig:stoch_simulation
[[file:./.ob-jupyter/e27105bfa4dbe01d563f05ff9102f3e8b6e43633.png]]

#+thumb:
#+begin_src jupyter-python :noweb yes :results output :eval never-export :exports none
V = {
    N: 100,
    I0: 10,
    alpha: 0.5,
    beta: 0.1,
    dt: 0.05
}
steps = 1000
times, n_hist = run_stoch_model(steps, V)
plot_stoch_model(times, n_hist, V)
plt.xticks([])
plt.yticks([])
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/f14840946652c492559c36799c34a902f862a42f.png]]

#+name: src:fig:stoch_quasi
#+begin_src jupyter-python :noweb yes :results output :eval never-export
V = {
    N: 100,
    S0: 95,
    I0: 5,
    n: 5,
    alpha: 0.8,
    beta: 0.6,
    dt: 0.05
}
V[I0] = int(V[N]*(V[alpha]-V[beta])/V[alpha])
times, n_hist = run_stoch_model(50000, V)
plot_stoch_model(times, n_hist, V)
plt.xlabel("$t$")
plt.ylabel("population")
plt.legend()
#+end_src

#+caption: A longer simulation of the stochastic system showing the increasing fluctuations that results in the infected population to die out.
#+attr_latex: scale=0.75
#+label: fig:stoch_quasi
#+RESULTS: src:fig:stoch_quasi
[[file:./.ob-jupyter/71ee1cabb99883a2350f8e6d17b5739a888f47d7.png]]

** Efficient simulation
We can make our simulation more efficient by realizing the similarity of the
algorithm to the one generating numbers from the [[file:20210315164132-exponential_distribution.org][exponential distribution]]. The
hypothesis is that we can simply sample from an exponential distribution to get
the time of the next event instead of simulating the whole system.

We will try and find the parameters for the distribution by simulating the
system and recording the event time deltas. See Figure [[fig:exponent_sims]] for the
results.

#+begin_src jupyter-python
bd_cases = [
    {b[n]: 0.1, d[n]: 0.2},
    {b[n]: 1.0, d[n]: 2.0},
    {b[n]: 10.0, d[n]: 5.0},
]
#+end_src

#+RESULTS:

#+begin_src jupyter-python :exports none
eq = LMatEq(rates_rhs, [Number(0.1), Number(0.2)])
eq
sol = solve([Eq(alpha*(1-n/N), b[n]), Eq(beta*n, d[n])], [alpha, beta])
alpha_bn = sol[alpha]
beta_bn = sol[beta]
LValues(sol)
#+end_src

#+RESULTS:
:RESULTS:
\begin{equation}\begin{cases}
\alpha = - \frac{N {b_{n}}}{- N + n}\\
\beta = \frac{{d_{n}}}{n}
\end{cases}\end{equation}
:END:

#+begin_src jupyter-python :noweb yes :results silent
def plot_dist_sims(axes, results, cases, bins='auto'):
    for i, (ts, case, axcol) in enumerate(zip(results, bd_cases, axes.T)):
        b_n = case[b[n]]
        d_n = case[d[n]]
        for ax, ti, lab, rate_txt, rate in zip(axcol, range(2), ['t_b', 't_d'], ['b_n', 'd_n'], case.values()):
            ax.set_title(latex(LValues(case, join=LComma), mode='inline'))
            hist, counts = np.histogram(ts[:, :, ti], bins=bins, density=True)
            t_lin = np.linspace(ts[:, :, ti].min(), ts[:, :, ti].max(), len(hist))
            (A, B), _ = curve_fit(lambda t,A,B: A*np.exp(-B*t), t_lin, hist, p0=(rate, rate))
            ax.plot(t_lin, A*np.exp(-B*t_lin), label=f"best fit$={A:.4f}e^{{{-B:.4f}t}}$", color=<<color("blue")>>)
            ax.plot(t_lin, rate*np.exp(-rate*t_lin), label=f"theoretical$={rate_txt}e^{{-{rate_txt}t}}$", color=<<color("blue")>>, ls='--')
            ax.scatter(t_lin, hist)
            ax.set_yscale('log')
            ax.set_xlabel(f"${lab}$")
            ax.set_ylabel(f"$log(P({lab}))$")
            ax.legend()
#+end_src


# Calculate by continuing time and taking differences
#+begin_src jupyter-python
def calc_times(prob_n, points, steps):
    times = np.zeros([steps, points, 2])
    t = np.zeros([points, 2])
    t_prev = np.zeros([points, 2])
    index = np.zeros([points, 2], dtype=int)
    n_n = np.zeros([points])
    k = 0
    while (index < steps).any():
        t += dt_n
        should_transition = np.random.random(size=[points, 2]) < prob_n
        for i in range(2):
            curr_trans = should_transition[:, i]&(index[:, i]<steps)
            if curr_trans.sum() > 0:
                curr_index = index[curr_trans, i]
                times[curr_index, curr_trans, i] = t[curr_trans, i]-t_prev[curr_trans, i]
                t_prev[curr_trans, i] = t[curr_trans, i]
                index[curr_trans, i] += 1
        k += 1
    return times
#+end_src

#+RESULTS:

#+name: src:fig:exponent_sims
#+begin_src jupyter-python :results output :noweb yes :eval never-export
results = []
points = 100
steps = 100
dt_n = 0.001
for case in bd_cases:
    b_n = case[b[n]]
    d_n = case[d[n]]
    results.append(calc_times(np.array([b_n*dt_n, d_n*dt_n]), points, steps))

fig, axs = plt.subplots(2, 3, figsize=(4*3, 4*2))
fig.suptitle(f"$dt={dt_n}$, samples=${points*steps}$")
plot_dist_sims(axs, results, bd_cases, bins=100)
#+end_src

#+caption: Simulations of three different sets of parameters, showing a comparison to the theoretical exponential distribution.
#+attr_latex: scale=0.75
#+label: fig:exponent_sims
#+RESULTS: src:fig:exponent_sims
[[file:./.ob-jupyter/7631f7314416378dfd0853697048676f0f0b8ce4.png]]



** Gillespie algorithm
Using the results from the previous section, we can now create a much faster
simulation using the [[file:20210309152203-gillespie_algorithm.org][Gillespie algorithm]], with this algorith we can just sample
the exponential distribution to get the time deltas.

*** Calculating $T_{ext}$
In this section we will calculate the time until extinction $T_{ext}$. We will
do this by running the simulation numerous times and recording when each
trajectory goes extinct to get an estimation. See Figure [[fig:t_extinction]] for
the results, the final value calculated was $T_{ext}\approx 289$.

#+begin_src jupyter-python :exports none
react = np.array([1, -1])
minval = np.array([0.0001])
def step(alpha, beta, N, n, t, alive):
    b_n = alpha*(1-n/N)*n
    d_n = beta*n
    alive &= (n > 0)
    Pb = -np.log(np.random.random(size=n.shape))/np.where(alive, b_n, minval)
    Pd = -np.log(np.random.random(size=n.shape))/np.where(alive, d_n, minval)
    P = np.stack([Pb, Pd]).T
    event = np.argmin(P, axis=1)
    dt = P[np.arange(len(n)), event]
    t[alive] += dt[alive]
    n[alive] += react[event[alive]]

n = np.zeros([10])
t = np.zeros([10])
alive = np.ones([10], dtype=bool)
step(0.6, 0.8, 100, n, t, alive)
t
#+end_src

#+RESULTS:
: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])

#+begin_src jupyter-python
def run_gillespie(steps, points, V, max_t=0):
    n_n = np.zeros([points])+V[I0]
    t_n = np.zeros([points])
    n_hist = np.zeros([steps, points])
    t_hist = np.zeros([steps, points])
    n_hist[0, :] = V[I0]
    alive = np.ones([points], dtype=bool)
    for i in range(steps):
        step(V[alpha], V[beta], V[N], n_n, t_n, alive)
        t_hist[i] = t_n
        n_hist[i, alive] = n_n[alive]
    return t_hist, n_hist
#+end_src

#+RESULTS:

#+begin_src jupyter-python :results silent
def run_gillespie_until(max_t, points, V):
    n_n = np.zeros([points])+V[I0]
    t_n = np.zeros([points])
    n_hist = np.zeros([steps, points])
    t_hist = np.zeros([steps, points])
    n_hist[0, :] = V[I0]
    alive = np.ones([points], dtype=bool)
    i = 0
    for i in range(steps):
        step(V[alpha], V[beta], V[N], n_n, t_n, alive)
        t_hist[i] = t_n
        n_hist[i, alive] = n_n[alive]
        if t_n.max() > max_t:
            return t_hist[:i], n_hist[i:]
        i += 1
    return t_hist, n_hist
#+end_src

#+begin_src jupyter-python :results silent
def run_gillespie_dead(points, V):
    n_n = np.zeros([points])+V[I0]
    t_n = np.zeros([points])
    alive = np.ones([points], dtype=bool)
    n_alive = points
    i = 0
    while n_alive > 0:
        step(V[alpha], V[beta], V[N], n_n, t_n, alive)
        n_alive = alive.sum()
        i += 1
    return t_n, n_n
#+end_src

#+thumb:
#+begin_src jupyter-python :noweb yes :results output :eval never-export :exports none
V = {
    N: 200,
    alpha: 0.8,
    beta: 0.6,
}
V[I0] = int(V[N]*(V[alpha]-V[beta])/V[alpha])
V[S0] = V[N] - V[I0]
steps = 40000
t_hist, n_hist = run_gillespie(steps, 10, V)
dead = n_hist[-1, :] == 0
plt.figure(figsize=(4, 4))
plt.plot(t_hist[:, dead], n_hist[:, dead], color=<<color("red")>>, lw=0.8, alpha=0.6)
plt.axhline(0, ls='--', color=<<color("fg-hc")>>, alpha=0.4, lw=0.5)
plt.scatter(t_hist[-1, dead], n_hist[-1, dead], marker='x', s=8)
plt.xticks([])
plt.yticks([])
T_ext = np.mean(t_n)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/06ebb8b1bc1ac2c8ce9748138b0ef6daa2905a66.png]]


#+name: src:t_extinction
#+begin_src jupyter-python :noweb yes :results output :eval never-export
V = {
    N: 100,
    alpha: 0.8,
    beta: 0.6,
}
V[I0] = int(V[N]*(V[alpha]-V[beta])/V[alpha])
V[S0] = V[N] - V[I0]
steps = 40000
trajectories = 10000
t_hist, n_hist = run_gillespie(steps, 100, V)
t_n, n_n = run_gillespie_dead(trajectories, V)
dead = n_hist[-1, :] == 0
plt.figure(figsize=(4, 4))
# plt.plot(t_hist, V[N]-n_hist, color=<<color("green")>>, lw=0.5, alpha=0.5)
plt.plot(t_hist[:, dead], n_hist[:, dead], color=<<color("red")>>, lw=0.4, alpha=0.1)
plt.axhline(0, ls='--', color=<<color("fg-hc")>>, alpha=0.4, lw=0.5)
density = gaussian_kde(t_n)
# plt.scatter(t_hist[-1, dead], n_hist[-1, dead], s=8)
dens_t = np.linspace(0, t_hist.max(), 200)
plt.xlabel("$t$")
plt.ylabel("population")
axdens = plt.gca().twinx()
dens = density(dens_t)
axdens.plot(dens_t, dens, label="accumulated death density")
axdens.set_ylabel("death density")
plt.title(latex(LValues(V, join=LComma), mode='inline'))
T_ext = np.mean(t_n)
axdens.axvline(T_ext, lw=0.6, color=<<color("blue")>>, label=f"mean $T_{{ext}}\\approx {T_ext:.2f}$")
plt.legend()
#+end_src

#+caption: A simulation of $10000$ trajectories showing the distribution of extinction events.
#+attr_latex: scale=0.75
#+label: fig:t_extinction
#+RESULTS: src:t_extinction
[[file:./.ob-jupyter/63059e743b5247ba2e59758300ec65fd0e97ec17.png]]


#+begin_src jupyter-python :exports none :eval never-export
V = {
    N: 100,
    alpha: 0.8,
    beta: 0.6,
}
V[I0] = int(V[N]*(V[alpha]-V[beta])/V[alpha])
V[S0] = V[N] - V[I0]
times = [5, T_ext, T_ext*100]
trajectories = 10000
results = [run_gillespie_until(t, trajectories, V) for i, t in enumerate(times)]
print("Done")
#+end_src

#+RESULTS:
: Done

*** Distribution $P(n_t)$
We will now find $P(n_t)$ by running the simulation until some time $t$ and
calculating the distribution of $n_t$. See Figure [[fig:prob_n]] for the results.

#+name: src:fig:prob_n
#+begin_src jupyter-python :eval never-export :results output :noweb yes
plt.figure(figsize=(3*4, 4))
def gaussian(x, a, x0, sigma):
    return a * np.exp(-(x - x0)**2 / (2 * sigma**2))

for i, (t_hist, n_hist) in enumerate(results):
    plt.subplot(1, 3, i+1)
    hist, bins = np.histogram(n_hist, bins=range(0, V[N]), density=True)
    hist = hist[1:]
    bins = bins[2:]
    plt.scatter(bins, 1/np.where(hist==0, np.nan, hist))
    t_max = t_hist[-1, :].max()
    plt.title(f"t={t_max:.2f}")
    t_norm = np.linspace(0, V[N], 200)
    mean = np.sum(bins*hist)/np.sum(hist)
    sigma = np.sqrt(np.sum(hist*(bins-mean)**2)/np.sum(hist))
    a, b = curve_fit(gaussian, bins, hist, p0=[hist.max(), mean, sigma])
    gaussian_fit = gaussian(t_norm, *a)
    plt.plot(t_norm, 1/np.where(gaussian_fit == 0, np.nan, gaussian_fit), color=<<color("blue")>>, label="gaussian fit", lw=1.0)
    plt.axvline(V[I0], color=<<color("red")>>, label="steady state", lw=1.0)
    plt.xlabel("$n_t$")
    plt.ylabel("$P(n_t)$")
    plt.xlim([0, V[N]])
    plt.ylim([10, 10e9])
    plt.yscale('log')
    plt.legend()
#+end_src

#+caption: Three simulations showing the distribution $P(n_t)$, one at an early state, one at $t=T_{\text{ext}}$ and one greater than $T_{\text{ext}}$.
#+attr_latex: scale=0.75
#+label: fig:prob_n
#+RESULTS: src:fig:prob_n
[[file:./.ob-jupyter/7c13efc6cc96d0731c79fbe1727e156b1d0b4b1d.png]]
