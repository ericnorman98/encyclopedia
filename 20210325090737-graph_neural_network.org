#+title: Graph neural network
#+roam_tags:
#+property: header-args:jupyter-python :tangle graph_neural_network.py
#+latex_header: \usepackage{fancyvrb}
#+latex_header: \fvset{frame=leftline,framesep=2mm,fontfamily=courier,fontsize=\scriptsize,numbers=left,framerule=.3mm,numbersep=1mm,commandchars=\\\{\}}
#+latex_header: \renewenvironment{verbatim}
#+latex_header:  {\SaveVerbatim{cverb}}
#+latex_header:  {\endSaveVerbatim
#+latex_header:   \flushleft\fboxrule=0pt\fboxsep=.5em\fvset{frame=leftline}
#+latex_header:   \colorbox{bgalt}{%
#+latex_header:     \makebox[\dimexpr\linewidth-2\fboxsep][l]{\BUseVerbatim{cverb}}%
#+latex_header:   }
#+latex_header:   \endflushleft
#+latex_header: }

- tags :: [[file:20210325091024-machine_learning.org][Machine learning]], [[file:20210224212626-graph_theory.org][Graph theory]]

#+call: init(theme='dark)

#+RESULTS:

#+begin_src jupyter-python :results silent
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import pytorch_lightning as pl
from torch.utils.data import DataLoader, random_split
from pytorch_lightning.callbacks.early_stopping import EarlyStopping
from encyclopedia.graph_theory import draw_graph
import torch_geometric.transforms as T
from torch_geometric.datasets import Planetoid
from torch_geometric.nn import GCNConv, GATConv
from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp
import torch_geometric.data as gdata
import torch_geometric.nn as gnn
import networkx as nx
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import sympy as sm
import logging
import warnings
warnings.filterwarnings('ignore')
logging.getLogger("lightning").setLevel(logging.ERROR)
logging.getLogger("lightning").addHandler(logging.NullHandler())
logging.getLogger("lightning").propagate = False
#+end_src

* Graph neural network
A graph neural network (GNN) is a [[file:20210410172737-neural_network.org][neural network]] that operates on graphs, this
lets us use the powerful concepts from [[file:20210224212626-graph_theory.org][graph theory]] to model relationships and
dependencies between pieces of information.

A graph neural network can have a lot of different types of layers just like any
other neural network model. You can for example have [[file:20210411092718-graph_convolution.org][graph convolution]] layers
and [[file:20210411092745-graph_pooling.org][graph pooling]] layers that transforms either the features of the graph or the
graph itself.

#+begin_src jupyter-python :results silent :exports none
from sklearn.manifold import TSNE
def visualize(out, color):
    z = TSNE(n_components=2).fit_transform(out.detach().cpu().numpy())

    plt.xticks([])
    plt.yticks([])

    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap="Set2")
#+end_src

#+begin_src jupyter-python :exports none
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device
#+end_src

#+RESULTS:
: device(type='cpu')

#+begin_src jupyter-python :exports none
dataset_name = 'Cora'
path = os.path.join(os.path.realpath('.'), 'data', dataset_name)
path
#+end_src

#+RESULTS:
: /home/eric/encyclopedia/data/Cora

#+begin_src jupyter-python :exports none
cora_dataset = Planetoid(path, dataset_name, transform=T.NormalizeFeatures())
cora_data = cora_dataset[0]
cora_data
#+end_src

#+RESULTS:
: Data(edge_index=[2, 10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])

** Cora dataset
We will demonstrate the power of graph neural network with some examples,
starting with the Cora dataset. The Cora dataset contains 2708 scientific
publications with seven different classifications. The set is also split into
training, validation and test subsets. See Table [[tab:cora_splits]] for the number
of nodes in each set.

#+name: src:tab:cora_splits
#+begin_src jupyter-python
[
    ["Set", "# of labeled nodes"], None,
    ["Training", int(cora_data.train_mask.sum())],
    ["Validation", int(cora_data.val_mask.sum())],
    ["Test", int(cora_data.test_mask.sum())],
]
#+end_src

#+name: tab:cora_splits
#+RESULTS: src:tab:cora_splits
| Set        | # of labeled nodes |
|------------+--------------------|
| Training   |                140 |
| Validation |                500 |
| Test       |               1000 |
The low number of labeled nodes in the training set makes this task more suited
to semi-supervised learning when classifying nodes, as opposed to supervised
learning.

#+begin_src jupyter-python :results silent
def get_mask(data, mode='train'):
    if mode == 'train':
        return data.train_mask
    elif mode == 'val':
        return data.val_mask
    elif mode == 'test':
        return data.test_mask
    else:
        raise Exception(f"Invalid forward mode: {mode}, expected one of: train, val, test")
#+end_src

#+begin_src jupyter-python :results silent
class GraphModel(pl.LightningModule):
    def __init__(self):
        super(GraphModel, self).__init__()
        torch.manual_seed(12345)

    def forward_all(self, data):
        # implemented by children
        pass

    def forward(self, data, mode='train'):
        mask = get_mask(data, mode)
        return self.forward_all(data)[mask]

    def training_step(self, data, batch_idx):
        x_hat = self.forward(data, mode='train')
        loss = F.cross_entropy(x_hat, data.y[data.train_mask])
        self.log('train_loss', loss)
        train_acc = self.calculate_accuracy(x_hat, data.y, data.train_mask)
        self.log('train_acc', train_acc)
        return loss

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=0.01, weight_decay=5e-4)
        return optimizer

    def validation_step(self, data, batch_idx):
        x_hat = self.forward(data, mode='val')
        loss = F.cross_entropy(x_hat, data.y[data.val_mask])
        self.log('val_loss', loss)
        val_acc = self.calculate_accuracy(x_hat, data.y, data.val_mask)
        self.log('val_acc', val_acc)

    def calculate_accuracy(self, x_hat, y, mask):
        pred = x_hat.argmax(dim=1)
        correct = pred == y[mask]
        return int(correct.sum()) / int(mask.sum())

    def test_step(self, data, batch_idx):
        x_hat = self.forward(data, mode='test')
        test_acc = self.calculate_accuracy(x_hat, data.y, data.test_mask)
        self.log('test_acc', test_acc)
#+end_src

#+begin_src jupyter-python :results silent
class MetricHistoryCallback(pl.callbacks.Callback):
    def __init__(self, metrics, *args, epochs_per_print=20, **kwargs):
        super().__init__(*args, **kwargs)
        self.metrics = metrics
        self.epochs_per_print = epochs_per_print
        self.history = {k: [] for k in metrics}

    def on_train_epoch_end(self, trainer, *args, **kwargs):
        for metric in self.metrics:
            m = trainer.callback_metrics.get(metric)
            if m:
                self.history[metric].append(m.detach().numpy())

        if trainer.current_epoch % self.epochs_per_print == 0:
            info = ', '.join([f"{k}: {float(v[-1]):.3f}" for k,v in self.history.items() if len(v) > 0])
            print(f"Epoch {trainer.current_epoch}/{trainer.max_epochs} - {info}")

    def on_test_epoch_end(self, trainer, *args, **kwargs):
        for metric in self.metrics:
            m = trainer.callback_metrics.get(metric)
            if m:
                self.history[metric].append(m.detach().numpy())
#+end_src

In the implementation, we begin by defining a function for training a model for
the Cora dataset, here we use [[https://github.com/PyTorchLightning/pytorch-lightning][PyTorch Lightning]] for the training.
#+begin_src jupyter-python :results silent :exports code
def train_cora_model(model, patience=10, max_epochs=500):
  history_cb = MetricHistoryCallback(['train_loss', 'val_loss',
                                      'test_acc', 'train_acc', 'val_acc'])
  trainer = pl.Trainer(max_epochs=max_epochs,
                       progress_bar_refresh_rate=0,
                       weights_summary=None,
                       callbacks=[EarlyStopping(monitor='val_loss',
                                                mode='min',
                                                patience=patience),
                                  history_cb])
  node_data_loader = gdata.DataLoader(cora_dataset, batch_size=1)
  print("Training...")
  trainer.fit(model, node_data_loader, node_data_loader)
  trainer.test(model, test_dataloaders=node_data_loader, verbose=False)
  print("Training Complete.")
  return history_cb.history
#+end_src
Here we also use [[file:20210410183611-early_stopping.org][early stopping]] with a patience of 10 epochs to improve the accuracy
on the test and to avoid [[file:20210411100933-overfitting.org][overfitting]].

#+begin_src jupyter-python :results silent
def create_results_table(results):
    train_loss = round(float(results['train_loss'][-1]), 4)
    val_loss = round(float(results['val_loss'][-1]), 4)
    test_acc = round(float(results['test_acc'][-1])*100, 4)
    return [["Training loss", "Validation loss", "Test accuracy"], None,
            [train_loss, val_loss, f"{test_acc}%"]]
#+end_src

#+begin_src jupyter-python :results silent
def plot_results(model_name, results):
    plt.subplot(1, 2, 1)
    plt.title("Loss")
    plt.plot(results['train_loss'], label="Training loss")
    plt.plot(results['val_loss'], label="Validation loss")
    plt.legend()
    plt.subplot(1, 2, 2)
    plt.title("Accuracy")
    plt.plot(results['train_acc'], label="Training accuracy")
    plt.plot(results['val_acc'], label="Validation accuracy")
    plt.suptitle(f"{model_name} training")
    plt.legend()
#+end_src

#+begin_src jupyter-python :results silent
def plot_node_embeddings(model_name, before_model, trained_model):
    plt.subplot(1, 2, 1)
    before_model.eval()
    out = before_model.forward_all(cora_data)
    visualize(out, color=cora_data.y)
    plt.title("Before training")
    plt.subplot(1, 2, 2)
    trained_model.eval()
    out = trained_model.forward_all(cora_data)
    visualize(out, color=cora_data.y)
    plt.title("After training")
    plt.suptitle(model_name)
#+end_src

*** Supervised MLP
We will start by training a standard multi-layer perceptron (MLP) using
supervised learning. The model will have two layers with ReLU activation and
dropout.

The implementation of the model looks like the following
#+begin_src jupyter-python :results silent :exports code
class MLP(GraphModel):
    def __init__(self, hidden_channels, num_features, num_classes):
        super(MLP, self).__init__()
        self.lin1 = nn.Linear(num_features, hidden_channels)
        self.lin2 = nn.Linear(hidden_channels, num_classes)

    def forward_all(self, data):
        x = data.x
        x = F.relu(self.lin1(x))
        x = F.dropout(x, p=0.5, training=self.training)
        x = self.lin2(x)
        return x
#+end_src
here the class inherits from =GraphModel= that removes some boilerplate, see
Appendix [[app:code]] for the full code.

Now we can start training the model using the training function defined
previously
#+begin_src jupyter-python :exports both
mlp_model = MLP(hidden_channels=16,
                num_features=cora_dataset.num_features,
                num_classes=cora_dataset.num_classes)
mlp_results = train_cora_model(mlp_model)
#+end_src

#+RESULTS:
#+begin_example
Training...
Epoch 0/500 - train_loss: 1.961, val_loss: 2.036, train_acc: 0.143, val_acc: 0.114
Epoch 20/500 - train_loss: 1.745, val_loss: 1.871, train_acc: 0.600, val_acc: 0.572
Epoch 40/500 - train_loss: 1.255, val_loss: 1.726, train_acc: 0.771, val_acc: 0.570
Epoch 60/500 - train_loss: 0.888, val_loss: 1.565, train_acc: 0.864, val_acc: 0.596
Epoch 80/500 - train_loss: 0.780, val_loss: 1.458, train_acc: 0.836, val_acc: 0.588
Epoch 100/500 - train_loss: 0.636, val_loss: 1.408, train_acc: 0.843, val_acc: 0.596
Epoch 120/500 - train_loss: 0.499, val_loss: 1.357, train_acc: 0.900, val_acc: 0.608
Epoch 140/500 - train_loss: 0.456, val_loss: 1.345, train_acc: 0.893, val_acc: 0.604
Epoch 160/500 - train_loss: 0.457, val_loss: 1.332, train_acc: 0.886, val_acc: 0.596
Epoch 180/500 - train_loss: 0.407, val_loss: 1.312, train_acc: 0.907, val_acc: 0.596
Epoch 200/500 - train_loss: 0.439, val_loss: 1.301, train_acc: 0.864, val_acc: 0.612
Training Complete.
#+end_example

You can see the final loss and accuracy from training this model in Table
[[tab:mlp_train_results]] and the full progress of training in Figure
[[fig:mlp_train_results]]. See Figure [[fig:mlp_node_embed]] for a visualization of the
node embeddings before and after training.

#+name: src:tab:mlp_train_results
#+begin_src jupyter-python
create_results_table(mlp_results)
#+end_src

#+name: tab:mlp_train_results
#+caption: MLP model training results.
#+RESULTS: src:tab:mlp_train_results
| Training loss | Validation loss | Test accuracy |
|---------------+-----------------+---------------|
|        0.3887 |          1.3027 |         58.9% |

#+name: src:fig:mlp_train_results
#+begin_src jupyter-python :results output
plt.figure(figsize=(8, 4))
plot_results("MLP", mlp_results)
#+end_src

#+name: fig:mlp_train_results
#+caption: MLP model training loss and accuracy progress plots.
#+RESULTS: src:fig:mlp_train_results
[[file:./.ob-jupyter/d8dd4a52b55c1d1af4af0c3774c8640ae1b2f653.png]]

#+name: src:fig:mlp_node_embed
#+begin_src jupyter-python :results output
plt.figure(figsize=(8, 4))
plot_node_embeddings("MLP", MLP(hidden_channels=16,
                                num_features=cora_dataset.num_features,
                                num_classes=cora_dataset.num_classes), mlp_model)
#+end_src

#+name: fig:mlp_node_embed
#+caption: MLP model t-distributed Stochastic Neighbor Embedding plot before and after training.
#+RESULTS: src:fig:mlp_node_embed
[[file:./.ob-jupyter/50e2ddafedf96904427a58dde982170060d88a1b.png]]

#+latex: \pagebreak
*** Semi-supervised GCN
We will now try a semi-supervised graph convolutional network, since we have a
limited amount of labels for the nodes we can make use of the structure of the
graph itself to help the network to classify the nodes. We get the structure
information, i.e. the edges, from the =edge_index= field from the Cora dataset.

The definition of the GCN model looks like this
#+begin_src jupyter-python :results silent :exports code
class GCN(GraphModel):
    def __init__(self, hidden_channels, num_features, num_classes):
        super(GCN, self).__init__()
        torch.manual_seed(12345)
        self.conv1 = GCNConv(num_features, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, num_classes)

    def forward_all(self, data):
        x, edge_index = data.x, data.edge_index
        x = F.relu(self.conv1(x, edge_index))
        x = F.dropout(x, p=0.5, training=self.training)
        x = self.conv2(x, edge_index)
        return x
#+end_src
it is as similar as possible to the MLP model but now we are using =GCNConv=
layers instead of linear layers.

Now we train it just like before
#+begin_src jupyter-python :exports both
gcn_model = GCN(hidden_channels=16,
                num_features=cora_dataset.num_features,
                num_classes=cora_dataset.num_classes)
gcn_results = train_cora_model(gcn_model)
#+end_src

#+RESULTS:
#+begin_example
Training...
Epoch 0/500 - train_loss: 1.945, val_loss: 1.946, train_acc: 0.271, val_acc: 0.232
Epoch 20/500 - train_loss: 1.666, val_loss: 1.785, train_acc: 0.829, val_acc: 0.754
Epoch 40/500 - train_loss: 1.247, val_loss: 1.520, train_acc: 0.850, val_acc: 0.780
Epoch 60/500 - train_loss: 0.878, val_loss: 1.262, train_acc: 0.900, val_acc: 0.788
Epoch 80/500 - train_loss: 0.659, val_loss: 1.087, train_acc: 0.943, val_acc: 0.786
Epoch 100/500 - train_loss: 0.491, val_loss: 0.986, train_acc: 0.964, val_acc: 0.788
Epoch 120/500 - train_loss: 0.423, val_loss: 0.920, train_acc: 0.971, val_acc: 0.794
Epoch 140/500 - train_loss: 0.401, val_loss: 0.872, train_acc: 0.964, val_acc: 0.792
Epoch 160/500 - train_loss: 0.339, val_loss: 0.847, train_acc: 0.979, val_acc: 0.796
Epoch 180/500 - train_loss: 0.316, val_loss: 0.822, train_acc: 0.986, val_acc: 0.786
Epoch 200/500 - train_loss: 0.304, val_loss: 0.819, train_acc: 0.979, val_acc: 0.796
Training Complete.
#+end_example

As before, you can see the final loss and accuracy from training this model in
Table [[tab:gcn_train_results]] and the full progress of training in Figure
[[fig:gcn_train_results]]. We can see a significant improvement over the MLP model.
See Figure [[fig:gcn_node_embed]] for a visualization of the node embeddings before
and after training.

#+name: src:tab:gcn_train_results
#+begin_src jupyter-python
create_results_table(gcn_results)
#+end_src

#+name: tab:gcn_train_results
#+caption: GCN final training results.
#+RESULTS: src:tab:gcn_train_results
| Training loss | Validation loss | Test accuracy |
|---------------+-----------------+---------------|
|        0.2834 |          0.7948 |         80.2% |

#+name: src:fig:gcn_train_results
#+begin_src jupyter-python :results output
plt.figure(figsize=(8, 4))
plot_results("GCN", gcn_results)
#+end_src

#+name: fig:gcn_train_results
#+caption: GCN model training loss and accuracy progress plots.
#+RESULTS: src:fig:gcn_train_results
[[file:./.ob-jupyter/71601698189922e24b8c89f163821494b534c0e3.png]]


#+name: src:fig:gcn_node_embed
#+begin_src jupyter-python :results output
plt.figure(figsize=(8, 4))
plot_node_embeddings("GCN", GCN(hidden_channels=16,
                                num_features=cora_dataset.num_features,
                                num_classes=cora_dataset.num_classes), gcn_model)
#+end_src

#+name: fig:gcn_node_embed
#+caption: GCN model t-distributed Stochastic Neighbor Embedding plot before and after training.
#+RESULTS: src:fig:gcn_node_embed
[[file:./.ob-jupyter/62f5e2d28b487d14534bb2392bfb17ed5801f066.png]]

#+latex: \pagebreak
*** Semi-supervised GAT
We will now try and outperform the previous results by using graph attention networks.

The model we will use looks like this
#+begin_src jupyter-python :results silent :exports code
class GAT(GraphModel):
    def __init__(self, hidden_channels, num_features, num_classes, heads=8):
        super(GAT, self).__init__()
        self.gat1 = GATConv(num_features, hidden_channels, dropout=0.6, heads=heads)
        self.gat2 = GATConv(heads*hidden_channels, hidden_channels, dropout=0.6, heads=1)

    def forward_all(self, data):
        x, edge_index, batch = data.x, data.edge_index, data.batch
        x = F.leaky_relu(self.gat1(x, edge_index))
        x = self.gat2(x, edge_index)
        return x
#+end_src

#+begin_src jupyter-python :results output :exports both
gat_model = GAT(hidden_channels=16,
                num_features=cora_dataset.num_features,
                num_classes=cora_dataset.num_classes)
gat_results = train_cora_model(gat_model)
#+end_src

#+RESULTS:
#+begin_example
Training...
Epoch 0/500 - train_loss: 2.773, val_loss: 2.775, train_acc: 0.050, val_acc: 0.016
Epoch 20/500 - train_loss: 1.624, val_loss: 1.675, train_acc: 0.771, val_acc: 0.740
Epoch 40/500 - train_loss: 1.064, val_loss: 1.170, train_acc: 0.836, val_acc: 0.808
Epoch 60/500 - train_loss: 0.777, val_loss: 0.941, train_acc: 0.871, val_acc: 0.804
Epoch 80/500 - train_loss: 0.805, val_loss: 0.868, train_acc: 0.836, val_acc: 0.786
Epoch 100/500 - train_loss: 0.584, val_loss: 0.772, train_acc: 0.871, val_acc: 0.804
Training Complete.
#+end_example

See Table [[tab:gat_results]] and Figure [[fig:gat_results]] for the results. We can see
a small but significant improvement over the GCN model which would suggest that
attention is useful for this task. See Figure [[fig:gat_node_embed]] for the node
embeddings.

#+name: src:tab:gat_results
#+begin_src jupyter-python
create_results_table(gat_results)
#+end_src

#+name: tab:gat_results
#+caption: Final results of the GAT model.
#+RESULTS: src:tab:gat_results
| Training loss | Validation loss | Test accuracy |
|---------------+-----------------+---------------|
|        0.6267 |          0.7408 |         83.3% |

#+name: src:fig:gat_results
#+begin_src jupyter-python :results output
plt.figure(figsize=(8, 4))
plot_results("GAT", gat_results)
#+end_src

#+name: fig:gat_results
#+caption: Loss and accuracy training progress plots for the GAT model.
#+RESULTS: src:fig:gat_results
[[file:./.ob-jupyter/91e2e9e5367c0212334e60221ff7911086e63d3b.png]]


#+name: src:fig:gat_node_embed
#+begin_src jupyter-python :results output
plt.figure(figsize=(8, 4))
plot_node_embeddings("GAT", GAT(hidden_channels=16,
                                num_features=cora_dataset.num_features,
                                num_classes=cora_dataset.num_classes), gat_model)
#+end_src

#+name: fig:gat_node_embed
#+caption: Node embeddings for the GAT model before and after training.
#+RESULTS: src:fig:gat_node_embed
[[file:./.ob-jupyter/87c65c95480a166334186bd2847d247ec071c44b.png]]

#+latex: \pagebreak
** Quantum bit dataset
We will now do graph classification using a dataset of simulated measurements of
a quantum bit memory. See Figure [[fig:qbit_graph_sample]] for a visualization of a
sample of graphs taken from the training set.

#+begin_src jupyter-python
qdata = torch.load('data/graphs.pt')
np.random.shuffle(qdata)
test_split = 0.2
valid_split = 0.1
test_len = int(len(qdata)*test_split)
train_len = len(qdata)-test_len
valid_len = int(train_len*valid_split)
train_len -= valid_len
test_set = qdata[:test_len]
valid_set = qdata[test_len:test_len+valid_len]
train_set = qdata[test_len+valid_len:]

[
    ["Set", "# of graphs"], None,
    ["Training", train_len],
    ["Validation", valid_len],
    ["Test", test_len],
]
#+end_src

#+RESULTS:
| Set        | # of graphs |
|------------+-------------|
| Training   |        2880 |
| Validation |         320 |
| Test       |         800 |

#+name: src:fig:qbit_graph_sample
#+begin_src jupyter-python :noweb yes
plt.figure(figsize=(4, 4))
for i in range(16):
    plt.subplot(4, 4, i+1)
    data = train_set[16+i]
    plt.title(np.argmax(data.y.tolist()), fontsize=6)
    plt.axis('off')
    G = nx.Graph()
    edges = list(zip(*data.edge_index.tolist(), data.edge_attr.tolist()))
    G.add_weighted_edges_from(edges)
    pos = nx.spring_layout(G)
    edges,weights = zip(*nx.get_edge_attributes(G,'weight').items())
    nx.draw_networkx(G, pos=pos,
            node_size=8,
            with_labels=False,
            node_color=<<color("green")>>,
            edge_color=weights,
            edge_vmin=0.0,
            edge_vmax=1.0,
            edge_cmap=plt.cm.summer)
    plt.subplots_adjust(hspace=.001)
#+end_src

#+name: fig:qbit_graph_sample
#+caption: A sample of graphs from the training set.
#+RESULTS: src:fig:qbit_graph_sample
[[file:./.ob-jupyter/2f0daf1c6bb1d7f1c077d265ec08591d58a6f870.png]]

#+begin_src jupyter-python :exports none
test_data_loader = gdata.DataLoader(test_set, batch_size=4)
i = iter(test_data_loader)
next(i)
next(i)
#+end_src

#+RESULTS:
: Batch(batch=[30], edge_attr=[262], edge_index=[2, 262], ptr=[5], x=[30, 2], y=[4, 4])

#+latex: \pagebreak
#+begin_src jupyter-python :results silent :exports code
class QModel(pl.LightningModule):
    def __init__(self, hidden_channels, num_features, num_classes):
        super(QModel, self).__init__()
        torch.manual_seed(12345)
        att_heads = 5
        layers = 3
        self.conv1 = gnn.GraphConv(num_features, hidden_channels)
        self.bn1 = gnn.BatchNorm(hidden_channels)

        self.conv2 = gnn.GraphConv(hidden_channels, hidden_channels*2)
        self.bn2 = gnn.BatchNorm(hidden_channels*2)
        self.conv3 = gnn.GraphConv(hidden_channels*2, hidden_channels*4)
        self.bn3 = gnn.BatchNorm(hidden_channels*4)
        self.conv4 = gnn.GraphConv(hidden_channels*4, hidden_channels*4)
        self.bn4 = gnn.BatchNorm(hidden_channels*4)

        self.mlp = nn.Sequential(
            nn.Linear(hidden_channels*4+2, hidden_channels),
            nn.Linear(hidden_channels, hidden_channels),
            nn.Linear(hidden_channels, num_classes)
        )

    def calculate_accuracy(self, x_hat, y):
        pred = x_hat.argmax(dim=1)
        targ = y.argmax(dim=1)
        correct = pred == targ
        return int(correct.sum()) / len(y)

    def forward(self, data):
        x, edge_index, edge_weight, batch = data.x, data.edge_index, data.edge_attr, data.batch
        x1 = x
        x = F.relu(self.conv1(x, edge_index, edge_weight=edge_weight))
        x = self.bn1(x)
        x = F.relu(self.conv2(x, edge_index, edge_weight=edge_weight))
        x = self.bn2(x)
        x = F.relu(self.conv3(x, edge_index, edge_weight=edge_weight))
        x = self.bn3(x)
        x = F.relu(self.conv4(x, edge_index, edge_weight=edge_weight))
        x = self.bn4(x)
        x = torch.cat([x, x1], dim=1)
        x = gmp(x, batch) # global max pooling
        x = self.mlp(x)
        x = F.log_softmax(x, dim=1)
        return x

    def training_step(self, data, batch_idx):
        x_hat = self.forward(data)
        loss = F.kl_div(x_hat, data.y)
        self.log('train_loss', loss)
        train_acc = self.calculate_accuracy(x_hat, data.y)
        self.log('train_acc', train_acc)
        return loss

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=0.01, weight_decay=5e-4)
        return optimizer

    def validation_step(self, data, batch_idx):
        x_hat = self.forward(data)
        loss = F.kl_div(x_hat, data.y)
        self.log('val_loss', loss)
        val_acc = self.calculate_accuracy(x_hat, data.y)
        self.log('val_acc', val_acc)

    def test_step(self, data, batch_idx):
        x_hat = self.forward(data)
        test_acc = self.calculate_accuracy(x_hat, data.y)
        self.log('test_acc', test_acc)
#+end_src

#+begin_src jupyter-python :results silent
def train_qbit_model(model):
  history_cb = MetricHistoryCallback(['train_loss', 'val_loss',
                                      'train_acc', 'val_acc', 'test_acc'],
                                     epochs_per_print=2)
  trainer = pl.Trainer(max_epochs=100,
                       progress_bar_refresh_rate=0,
                       weights_summary=None,
                       callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=10),
                                  history_cb])
  train_data_loader = gdata.DataLoader(train_set, batch_size=256, shuffle=True, num_workers=4)
  valid_data_loader = gdata.DataLoader(valid_set, batch_size=valid_len, shuffle=True)
  test_data_loader = gdata.DataLoader(test_set, batch_size=test_len)
  print("Training...")
  trainer.fit(model, train_data_loader, valid_data_loader)
  trainer.test(model, test_dataloaders=test_data_loader, verbose=False)
  print("Training complete.")
  return history_cb.history
#+end_src

#+begin_src jupyter-python :exports both
q_model = QModel(hidden_channels=64, num_features=2, num_classes=4).double()
results = train_qbit_model(q_model)
#+end_src

#+RESULTS:
#+begin_example
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
Training...
Epoch 0/100 - train_loss: 0.332, val_loss: 0.415, train_acc: 0.266, val_acc: 0.222
Epoch 2/100 - train_loss: 0.303, val_loss: 0.333, train_acc: 0.359, val_acc: 0.353
Epoch 4/100 - train_loss: 0.250, val_loss: 0.329, train_acc: 0.438, val_acc: 0.406
Epoch 6/100 - train_loss: 0.201, val_loss: 0.251, train_acc: 0.641, val_acc: 0.559
Epoch 8/100 - train_loss: 0.140, val_loss: 0.185, train_acc: 0.703, val_acc: 0.706
Epoch 10/100 - train_loss: 0.169, val_loss: 0.204, train_acc: 0.625, val_acc: 0.666
Epoch 12/100 - train_loss: 0.121, val_loss: 0.164, train_acc: 0.859, val_acc: 0.762
Epoch 14/100 - train_loss: 0.125, val_loss: 0.129, train_acc: 0.797, val_acc: 0.788
Epoch 16/100 - train_loss: 0.124, val_loss: 0.127, train_acc: 0.828, val_acc: 0.788
Epoch 18/100 - train_loss: 0.121, val_loss: 0.137, train_acc: 0.781, val_acc: 0.775
Epoch 20/100 - train_loss: 0.122, val_loss: 0.118, train_acc: 0.766, val_acc: 0.816
Epoch 22/100 - train_loss: 0.121, val_loss: 0.117, train_acc: 0.844, val_acc: 0.806
Epoch 24/100 - train_loss: 0.154, val_loss: 0.113, train_acc: 0.734, val_acc: 0.831
Epoch 26/100 - train_loss: 0.095, val_loss: 0.144, train_acc: 0.844, val_acc: 0.778
Epoch 28/100 - train_loss: 0.103, val_loss: 0.116, train_acc: 0.844, val_acc: 0.803
Epoch 30/100 - train_loss: 0.098, val_loss: 0.126, train_acc: 0.828, val_acc: 0.819
Epoch 32/100 - train_loss: 0.148, val_loss: 0.135, train_acc: 0.781, val_acc: 0.803
Epoch 34/100 - train_loss: 0.140, val_loss: 0.111, train_acc: 0.781, val_acc: 0.816
Epoch 36/100 - train_loss: 0.131, val_loss: 0.124, train_acc: 0.797, val_acc: 0.797
Epoch 38/100 - train_loss: 0.117, val_loss: 0.122, train_acc: 0.875, val_acc: 0.806
Epoch 40/100 - train_loss: 0.118, val_loss: 0.130, train_acc: 0.828, val_acc: 0.803
Epoch 42/100 - train_loss: 0.154, val_loss: 0.108, train_acc: 0.781, val_acc: 0.828
Epoch 44/100 - train_loss: 0.111, val_loss: 0.115, train_acc: 0.797, val_acc: 0.841
Epoch 46/100 - train_loss: 0.101, val_loss: 0.133, train_acc: 0.812, val_acc: 0.803
Training complete.
#+end_example

#+begin_src jupyter-python
create_results_table(results)
#+end_src

#+RESULTS:
| Training loss | Validation loss | Test accuracy |
|---------------+-----------------+---------------|
|        0.1014 |          0.1277 |        80.25% |

#+begin_src jupyter-python
plt.figure(figsize=(8, 4))
plot_results("Q-bit", results)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/c8e2eaf469c3d8e7082401c85cfcd08bca06ee26.png]]

#+latex: \appendix
#+latex: \pagebreak
* Code
<<app:code>>
#+include: graph_neural_network.py src python
