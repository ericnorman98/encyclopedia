#+title: Hypothesis test
#+roam_tags: statistics hypothesis test

- tags :: [[file:20210219102643-statistics.org][Statistics]]

#+call: init()

#+begin_src jupyter-python :lib yes
from sympy import *
from sympy.stats import *
from pyorg.latex import *
from encyclopedia.statistics import *
from encyclopedia.p_value import *
#+end_src

#+RESULTS:

#+begin_src jupyter-python :lib yes
alpha, mu0, H0, H1 = symbols('alpha mu_0 H_0 H_1')
#+end_src

#+RESULTS:

* Hypothesis test
#+begin_src jupyter-python :lib yes
def hypothesis_test(h0, h1, p_value, V):
    result = Piecewise((LColon(H0, h0), LGe(p_value[0], alpha)), (LColon(H1, h1), True))
    return LArray(
        result,
        LCalculation(
            p_value[0]<alpha,
            LLt(p_value[-1].doit().subs(V), V[alpha]),
            result.subs([(p, p_value[-1].subs(V).doit()), (alpha, V[alpha])]).doit(),
        join=limply),
    )
#+end_src

#+RESULTS:

#+begin_src jupyter-python
p_value = calculate_p_value(t, T, left_tailed)
hypothesis = hypothesis_test("the effect of interest is zero",
                             "the effect of interest is not zero",
                             p_value,
                             {alpha: 0.05})
hypothesis[0]
#+end_src

#+RESULTS:
:RESULTS:
\begin{equation}\begin{cases} H_{0}:\mathtt{\text{the effect of interest is zero}} & \text{for}\: p\geq \alpha \\H_{1}:\mathtt{\text{the effect of interest is not zero}} & \text{otherwise} \end{cases}\end{equation}
:END:

We reject the null hypothesis if the [[file:20210302194452-p_value.org][p-value]] is less than the /significance
level/ $\alpha$. To compute the p-value, use the observed test-statistic $t$ and
compare it to the unknown distribution $T$. There are three types of p-values

#+begin_src jupyter-python
LArray(
    Latex(calculate_p_value(t, T, left_tailed)[-1],  lquad, "one-sided left-tailed test"),
    Latex(calculate_p_value(t, T, right_tailed)[-1], lquad, "one-sided right-tailed test"),
    Latex(calculate_p_value(t, T, two_tailed)[-1],   lquad, "two-sided test"),
)
#+end_src

#+RESULTS:
:RESULTS:
\begin{equation}\begin{array}{l}
P[T \leq t]\quad \mathtt{\text{one-sided left-tailed test}}\\
P[T \geq t]\quad \mathtt{\text{one-sided right-tailed test}}\\
P[\left|{T}\right| \geq \left|{t}\right|]\quad \mathtt{\text{two-sided test}}
\end{array}\end{equation}
:END:
If we have a p-value of $0.01$, rejection of $H_0$ would look like this
#+begin_src jupyter-python
hypothesis[-1].replace(Probability, lambda e: 0.01)
#+end_src

#+RESULTS:
:RESULTS:
\begin{equation}\begin{aligned}
p < \alpha&=0.01<0.05=\\
&=H_{1}:\mathtt{\text{the effect of interest is not zero}}
\end{aligned}\end{equation}
:END:

| State of nature | Negative decision | Positive decision |
|-----------------+-------------------+-------------------|
| $H_0$ is true   | True negative     | Type I error      |
| $H_1$ is true   | Type II error     | True positive     |

** Example
#+begin_src jupyter-python
V = {
    alpha: 0.05,
    mu0: 3 + Rational(1,2),
    mu: 3 + Rational(1,10),
    sigma: 1 + Rational(1,2),
    xm: 3+Rational(1,10),
    s: 1 + Rational(1,2),
    n: 50
}
#+end_src

#+RESULTS:

Consider an experiment with a sample drawn from a [[file:20210225141719-normal_distribution.org][normal distribution]] with the
following results
#+begin_src jupyter-python
Latex({k: Float(v) for k,v in V.items() if k not in [mu, sigma, alpha, mu0]})
#+end_src

#+RESULTS:
:RESULTS:
\begin{equation}\begin{cases}
\bar{x}=3.1\\
s=1.5\\
n=50.0
\end{cases}\end{equation}
:END:

We use the test statistic
#+begin_src jupyter-python
V[xm] = 3 + Rational(1, 10)
t_eq = LEq(t, (xm-mu0)/(s/sqrt(n)))
t_eq
#+end_src

#+RESULTS:
:RESULTS:
\begin{equation}t=\frac{\sqrt{n} \left(\bar{x} - \mu_{0}\right)}{s}\end{equation}
:END:

We want to test the hypothesis
#+begin_src jupyter-python
T_eq = LEq(T, t_eq.rhs.subs({xm: Normal('\\bar{X}', V[mu0], V[s]/sqrt(V[n]))}))
p_value = calculate_p_value(t_eq.rhs, T_eq.rhs, left_tailed)
hypothesis_ex = hypothesis_test(Eq(mu0, V[mu0]), Not(Eq(mu0, V[mu0])), p_value, V)
hypothesis_ex[0]
#+end_src

#+RESULTS:
:RESULTS:
\begin{equation}\begin{cases} H_{0}:\mu_{0} = \frac{7}{2} & \text{for}\: p\geq \alpha \\H_{1}:\mu_{0} \neq \frac{7}{2} & \text{otherwise} \end{cases}\end{equation}
:END:

If $\bar{X}$ is the sample distribution, the test distribution becomes,
#+begin_src jupyter-python
T_eq
#+end_src

#+RESULTS:
:RESULTS:
\begin{equation}T=\frac{\sqrt{n} \left(- \mu_{0} + \bar{X}\right)}{s}\end{equation}
:END:

We can now calculate the p-value
#+begin_src jupyter-python
p_value.subs(V).evalf()[:-1].append(p_value[-1].subs(V).doit().evalf())
#+end_src

#+RESULTS:
:RESULTS:
\begin{equation}\begin{aligned}
p&=P[T \leq t]=\\
&=P[\frac{10 \sqrt{2} \left(\bar{X} - \frac{7}{2}\right)}{3} \leq - \frac{4 \sqrt{2}}{3}]=\\
&=0.0296732193959599
\end{aligned}\end{equation}
:END:

Checking the p-value against our significance level leads us to
#+begin_src jupyter-python
hypothesis_ex[-1].evalf()
#+end_src

#+RESULTS:
:RESULTS:
\begin{equation}\begin{aligned}
p < \alpha&\Rightarrow 0.0296732193959599<0.05\Rightarrow \\
&\Rightarrow H_{1}:\mu_{0} \neq 3.5
\end{aligned}\end{equation}
:END:

#+thumb:
#+begin_src jupyter-python :noweb yes :results output
x_n = np.linspace(-3, 0)
dens_lm = lambdify(x, density(T_eq.rhs)(x).subs(V))
plt.plot(x_n, dens_lm(x_n), zorder=2)
statf = t_eq.rhs.subs(V).evalf()
plt.axvline(statf, color=<<color("red")>>)
x_stat_n = np.linspace(-3, float(statf))
plt.fill_between(x_stat_n, 0, dens_lm(x_stat_n), color=<<color("blue")>>)
plt.xticks([float(statf)])
plt.yticks([0, 0.5])
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/fd36746a9d54aaf30253a06076965cc98b33eca4.png]]
